{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aman-Verma-10/NLP/blob/master/Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37ZYy9o0KL-g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "outputId": "94adcf73-2dd6-48f9-a9ef-cf9dd03bae3a"
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"popular\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2fjvYIHRIqp",
        "colab_type": "text"
      },
      "source": [
        "# **Word Tokenization-->**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfm8GP1yKg30",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "636e2b12-92f2-46e8-b641-854def577d22"
      },
      "source": [
        "from nltk.tokenize import word_tokenize as wt\n",
        "text = \"A.P.J. Abdul Kalam served as the 11th president of the Republic of India between 2002 and 2007. He was an inspiring teacher, and a narrative author, and a great scientist.. \"\n",
        "print(wt(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['A.P.J', '.', 'Abdul', 'Kalam','served', 'as', 'the', '11th', 'president', 'of', 'the', 'Republic', 'of', 'India', 'between', '2002', 'and', '2007', '.', 'He', 'was', 'an', 'inspiring', 'teacher', ',', 'and', 'a', 'narrative', 'author', ',', 'and', 'a', 'great', 'scientist..']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUopzGzwRqhS",
        "colab_type": "text"
      },
      "source": [
        "# **Tokenization of Paragraphs/Sentence-->**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdDvz_jXKmvx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "acd52a8f-8e49-4197-8af2-5f0f662c0c5f"
      },
      "source": [
        "import nltk\n",
        "\n",
        "\n",
        "paragraph = \"\"\" A.P.J. Abdul Kalam served as the 11th president of the Republic of India between 2002 and 2007. He was an inspiring teacher, and a narrative author, and a great scientist.. \"\"\"\n",
        "               \n",
        "# Tokenizing sentence\n",
        "sentence = nltk.sent_tokenize(paragraph)\n",
        "\n",
        "# Tokenizing words\n",
        "words = nltk.word_tokenize(paragraph)\n",
        "print(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[' A.P.J.', 'Abdul Kalam served as the 11th president of the Republic of India between 2002 and 2007.', 'He was an inspiring teacher, and a narrative author, and a great scientist..']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LuWskW7R17-",
        "colab_type": "text"
      },
      "source": [
        "# **Stop Words**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBHtCkMsKs7k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "9307fd48-a4cf-4865-d5df-21b2f36334fa"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JggtYoXKtXC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "e551fac8-9a81-4e92-e5ae-68224965e9f4"
      },
      "source": [
        "print(stopwords.words('english'))\n",
        "\n",
        "text = \" A.P.J. Abdul Kalam served as the 11th president of the Republic of India between 2002 and 2007. He was an inspiring teacher, and a narrative author, and a great scientist.. \"\n",
        "text_tokens = word_tokenize(text)\n",
        "\n",
        "tokens_without_sw = [word for word in text_tokens if not word in stopwords.words('english')]\n",
        "\n",
        "print(text_tokens)\n",
        "print(tokens_without_sw)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "['A.P.J', '.', 'Abdul', 'Kalam', 'served', 'as', 'the', '11th', 'president', 'of', 'the', 'Republic', 'of', 'India', 'between', '2002', 'and', '2007', '.', 'He', 'was', 'an', 'inspiring', 'teacher', ',', 'and', 'a', 'narrative', 'author', ',', 'and', 'a', 'great', 'scientist..']\n",
            "['A.P.J', '.', 'Abdul', 'Kalam', 'served', '11th', 'president', 'Republic', 'India', '2002', '2007', '.', 'He', 'inspiring', 'teacher', ',', 'narrative', 'author', ',', 'great', 'scientist..']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNFIpXjBSJrn",
        "colab_type": "text"
      },
      "source": [
        "# **Steming-->**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4lXmWVPKtdo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7995e8fb-d239-4787-809d-1ab542da32a6"
      },
      "source": [
        "from nltk.stem.porter import *\n",
        "\n",
        "porterStemmer = PorterStemmer()\n",
        "\n",
        "sentence=\"NLP is the new buzz word of the present era, it has added colours to lives of many new age entrepreneurs as a tool for personal mastery.\"\n",
        "wordsList = nltk.word_tokenize(sentence)\n",
        "\n",
        "stemWords = [porterStemmer.stem(word) for word in wordsList]\n",
        "\n",
        "print(' '.join(stemWords))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nlp is the new buzz word of the present era, it ha ad colour to live of mani new age entrepreneur as a tool for person masteri .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VASOMM5YSNXf",
        "colab_type": "text"
      },
      "source": [
        "# **Word Lemma**\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQ7oXAizKtjG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "88251660-d769-4162-d5d2-077951e33ff3"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "word_net_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "sentence = \"NLP is the new buzz word of the present era, it has added colours to lives of many new age entrepreneurs as a tool for personal mastery.\"\n",
        "punctuations=\"?:!.,;\"\n",
        "sentences_words = nltk.word_tokenize(sentence)\n",
        "for word in sentences_words:\n",
        "    if word in punctuations:\n",
        "        sentences_words.remove(word)\n",
        "\n",
        "sentences_words\n",
        "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
        "for word in sentences_words:\n",
        "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word                Lemma               \n",
            "NLP                 NLP                 \n",
            "is                  is                  \n",
            "the                 the                 \n",
            "new                 new                 \n",
            "buzz                buzz                \n",
            "word                word                \n",
            "of                  of                  \n",
            "the                 the                 \n",
            "present             present             \n",
            "era                 era                 \n",
            "it                  it                  \n",
            "has                 ha                  \n",
            "added               added               \n",
            "colours             colour              \n",
            "to                  to                  \n",
            "lives               life                \n",
            "of                  of                  \n",
            "many                many                \n",
            "new                 new                 \n",
            "age                 age                 \n",
            "entrepreneurs       entrepreneur        \n",
            "as                  a                   \n",
            "a                   a                   \n",
            "tool                tool                \n",
            "for                 for                 \n",
            "personal            personal            \n",
            "mastery             mastery             \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VL-6xIdCKtcw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "1925fb2c-23d4-4a40-dcdf-d76e4ec21925"
      },
      "source": [
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wnl = WordNetLemmatizer()\n",
        "txt = \"\"\"NLP is the practice of understanding how people organise their thinking, feeling, language and behaviour to produce the results they do.\"\"\"\n",
        "[wnl.lemmatize(i,j[0].lower()) if j[0].lower() in ['a','n','v'] else wnl.lemmatize(i) for i,j in pos_tag(word_tokenize(txt))]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NLP',\n",
              " 'be',\n",
              " 'the',\n",
              " 'practice',\n",
              " 'of',\n",
              " 'understand',\n",
              " 'how',\n",
              " 'people',\n",
              " 'organise',\n",
              " 'their',\n",
              " 'thinking',\n",
              " ',',\n",
              " 'feeling',\n",
              " ',',\n",
              " 'language',\n",
              " 'and',\n",
              " 'behaviour',\n",
              " 'to',\n",
              " 'produce',\n",
              " 'the',\n",
              " 'result',\n",
              " 'they',\n",
              " 'do',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fde145o2KtWR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "29c880b2-a655-4e31-b102-dd758826a1b7"
      },
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "text = \"NLP is the practice of understanding how people organise their thinking, feeling, language and behaviour to produce the results they do. \"\n",
        "TreebankWordTokenizer().tokenize(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NLP',\n",
              " 'is',\n",
              " 'the',\n",
              " 'practice',\n",
              " 'of',\n",
              " 'understanding',\n",
              " 'how',\n",
              " 'people',\n",
              " 'organise',\n",
              " 'their',\n",
              " 'thinking',\n",
              " ',',\n",
              " 'feeling',\n",
              " ',',\n",
              " 'language',\n",
              " 'and',\n",
              " 'behaviour',\n",
              " 'to',\n",
              " 'produce',\n",
              " 'the',\n",
              " 'results',\n",
              " 'they',\n",
              " 'do',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    }
  ]
}